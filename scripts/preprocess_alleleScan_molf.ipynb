{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for preprocessing alleleScan (ref) data \\\n",
    "Starting from the aligned pseudodiploid bams, we separate them out into wA 0,1,2 \\\n",
    "Then we get allele specific coverage and sequences accordingly \\\n",
    "We save this temporary result in a data_noGC_ref.h5 and then perform GC regression on it \\\n",
    "NOTE: This script only generates data_ident_ref.h5 (only ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General import, names of train, test, val\n",
    "import numpy as np\n",
    "import pysam\n",
    "from tqdm.notebook import tqdm\n",
    "import h5py\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "basedir = '/data/leslie/sunge/'\n",
    "aligndir = f'{basedir}f1_hybrid/EpiSC_ATAC_bam/'\n",
    "ctype = 'EpiSC'\n",
    "strain = 'molf'\n",
    "b6_strain = 'b6_molf'\n",
    "ident = ''     # ident is '' for you or whatever label you want to use for the h5 data you've generated\n",
    "\n",
    "datadir = f'{basedir}f1_ASA/{ctype}/data/{strain}/'\n",
    "chrom_train = [1,2,4,6,7,8,9,11,12,13,14,16,17,18,19]\n",
    "chrom_val = [3,5]\n",
    "chrom_test = [10,15]\n",
    "chroms = [chrom_train, chrom_val, chrom_test]\n",
    "name = ['train', 'val', 'test']\n",
    "\n",
    "reps = ['r1','r2','r3','r4']\n",
    "seqlen = 300                         # region around summit for sequence\n",
    "seqlen1 = 150                        # region around summit for coverage\n",
    "save = True                          # failsafe to prevent unwanted overwriting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing of Model Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# some utility functions\n",
    "from utils import get_summits, get_shifts, one_hot, GCregress\n",
    "\n",
    "# make data directory for the cell type - done ONCE\n",
    "if not os.path.exists(datadir):\n",
    "    os.makedirs(datadir)\n",
    "    # separate out replicate BAM files based on allele tag wA, to be used for pileup purposes\n",
    "    for rep in reps:\n",
    "        bamfile = aligndir+b6_strain+'/'+rep+'.combined.rmDup.Aligned.sortedByCoord.out.bam'\n",
    "        _ = pysam.index(bamfile)\n",
    "        bamf = pysam.AlignmentFile(bamfile, \"rb\")\n",
    "        temp1 = pysam.AlignmentFile(datadir+rep+'_b6.bam', 'wb', template=bamf)\n",
    "        temp2 = pysam.AlignmentFile(datadir+rep+'_molf.bam', 'wb', template=bamf)\n",
    "        temp3 = pysam.AlignmentFile(datadir+rep+'_both.bam', 'wb', template=bamf)\n",
    "        i=0\n",
    "        for read in bamf.fetch():\n",
    "            try:\n",
    "                idx = read.get_tag('wA')\n",
    "                if idx==0:\n",
    "                    temp1.write(read)\n",
    "                elif idx==1:\n",
    "                    temp2.write(read)\n",
    "                elif idx==2:\n",
    "                    temp3.write(read)\n",
    "            except:\n",
    "                i+=1\n",
    "        print(i)\n",
    "        temp1.close()\n",
    "        temp2.close()\n",
    "        temp3.close()\n",
    "        bamf.close()\n",
    "\n",
    "        _ = pysam.index(datadir+rep+'_b6.bam')\n",
    "        _ = pysam.index(datadir+rep+'_molf.bam')\n",
    "        _ = pysam.index(datadir+rep+'_both.bam')\n",
    "\n",
    "N = []\n",
    "for rep in reps:\n",
    "    bamfile = aligndir+b6_strain+'/'+rep+'.combined.rmDup.Aligned.sortedByCoord.out.bam'\n",
    "    bamf = pysam.AlignmentFile(bamfile, \"rb\")\n",
    "    N.append( sum([bamf.get_index_statistics()[i][1] for i in range(len(chrom_train+chrom_test+chrom_val))]) )\n",
    "    bamf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process peakatlas file to get peaks dataframe\n",
    "if ident[:3]=='_vi':   # This is us using a peak atlas, you would be using the else clause here\n",
    "    summits = pd.read_csv(aligndir+'cd8_old/yi_cd8_peaks_33143.csv',sep=',',index_col=0)\n",
    "    summits['start'] += seqlen//2\n",
    "    summits = summits.iloc[:,1:3]\n",
    "    summits.columns = range(2)\n",
    "    summits_neg = pd.read_csv(aligndir+'cd8_old/yi_cd8_uneg_peaks_33143.csv',sep=',',index_col=0)\n",
    "    summits_neg['start'] += seqlen//2\n",
    "    summits_neg = summits_neg.iloc[:,1:3].reset_index(drop=True)\n",
    "    summits_neg.columns = range(2)\n",
    "else:\n",
    "    if ident=='_yi':\n",
    "        peakfile = aligndir+'cd8_old/peaks_yi.bed'\n",
    "    else:\n",
    "        peakfile = aligndir+b6_strain+'/peaks/peakatlas.bed'\n",
    "    peaks = pd.read_csv(peakfile, sep='\\t', header=None)\n",
    "\n",
    "    if not ident:\n",
    "        # filter peaks based on IDR threshold (column 12 of BED)\n",
    "        idr_thresh = 0.05\n",
    "        idx = np.where( peaks.iloc[:,11] >= -np.log10(idr_thresh) )[0]\n",
    "        peaks = peaks.loc[idx].reset_index(drop=True)\n",
    "\n",
    "#     ## remove blacklist regions - try with and without\n",
    "#     blacklistfile = aligndir+'mm10-blacklist.v2.bed'\n",
    "#     blacklist = pd.read_csv(blacklistfile, sep='\\t', header=None)\n",
    "#     peaks = remove_blacklist(peaks, blacklist)\n",
    "\n",
    "    # get summits of called + flanking neg peaks (moving away from this to background unegs)\n",
    "    summits, summits_neg = get_summits(peaks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bc1a5458e24515ab9262d908616cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code to shortlist negative flanking summits to low accessibility - not needed if you are doing background unegs\n",
    "# Shit code, haven't used in forever, might need overhauling\n",
    "if ident[:3]!='_vi':\n",
    "    # FOR UNEGS, get the shortlist of summits_neg that have low accessibility\n",
    "    # get indices for each rep\n",
    "    idx = dict()\n",
    "    for rep in tqdm(reps):\n",
    "        bamfile = aligndir+b6_strain+'/'+rep+'.combined.rmDup.Aligned.sortedByCoord.out.bam'\n",
    "        bamf = pysam.AlignmentFile(bamfile, \"rb\")\n",
    "\n",
    "        idx[rep] = []\n",
    "        for chrom in chroms[:2]:\n",
    "            for c in chrom:\n",
    "                chromsummits = summits_neg.loc[np.where(summits_neg[0]==c)[0]]\n",
    "                y_uneg = []\n",
    "                for i in chromsummits[1]:\n",
    "                    y_uneg.append(bamf.count(str(c),i-seqlen//2,i+seqlen//2))\n",
    "                idx[rep] += list(chromsummits.index[ np.where(np.array(y_uneg)<5)[0] ])\n",
    "        bamf.close()\n",
    "\n",
    "    del y_uneg\n",
    "    # get low accessibility indices across reps\n",
    "    temp = set(idx[reps[0]])\n",
    "    for rep in reps[1:]:\n",
    "        temp.intersection_update(idx[rep])\n",
    "    idx = list(temp)\n",
    "\n",
    "    # shortlist unegs to low accessibility indexes\n",
    "    summits_neg = summits_neg.loc[idx].reset_index(drop=True)\n",
    "    frac=0.5\n",
    "    if len(summits_neg) > frac*len(summits):\n",
    "        idx = rng.choice(len(summits_neg), int(frac*len(summits)), replace=False)\n",
    "        summits_neg = summits_neg.loc[idx].reset_index(drop=True)\n",
    "    summits_neg = summits_neg.sort_values(by=[0,1], ignore_index=True)\n",
    "\n",
    "    if save:\n",
    "        summits.to_csv(datadir+'summits'+ident+'.csv', index=False, header=False)\n",
    "        summits_neg.to_csv(datadir+'summits_neg'+ident+'.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate y\n",
    "# Use summits and BAM files to generate pileup summed accessibilities\n",
    "y = dict()\n",
    "for rep in tqdm(reps):\n",
    "    temp1 = pysam.AlignmentFile(datadir+rep+'_b6.bam', 'rb')\n",
    "    temp2 = pysam.AlignmentFile(datadir+rep+'_molf.bam', 'rb')\n",
    "    temp3 = pysam.AlignmentFile(datadir+rep+'_both.bam', 'rb')\n",
    "\n",
    "    for n,chrom in zip(name,chroms):\n",
    "        y[rep+'_'+n+'_b6'] = []\n",
    "        y[rep+'_'+n+'_molf'] = []\n",
    "\n",
    "        for c in chrom:        # for each chromosome in train/val/test set\n",
    "            chromsummits = summits.iloc[np.where(summits[0]==c)[0],1]     # slice out the relevant chromosome summits\n",
    "            y_b6 = []\n",
    "            y_molf = []\n",
    "            for i in chromsummits:\n",
    "                # for each summit peak, get the summed accessibility from BAM pileup\n",
    "                y1 = temp1.count(str(c),i-seqlen1//2,i+seqlen1//2)\n",
    "                y2 = temp2.count(str(c),i-seqlen1//2,i+seqlen1//2)\n",
    "                y3 = temp3.count(str(c),i-seqlen1//2,i+seqlen1//2)\n",
    "                y_b6.append(y1+y3/2)\n",
    "                y_molf.append(y2+y3/2)            \n",
    "            \n",
    "            y[rep+'_'+n+'_b6'] += y_b6\n",
    "            y[rep+'_'+n+'_molf'] += y_molf\n",
    "    \n",
    "    temp1.close()\n",
    "    temp2.close()\n",
    "    temp3.close()\n",
    "    \n",
    "del y_b6, y_molf, y1, y2, y3, chromsummits\n",
    "\n",
    "# merge RPMs across reps\n",
    "for n in name:\n",
    "    y[n+'_b6'] = np.zeros_like(y[rep+'_'+n+'_b6'])\n",
    "    y[n+'_molf'] = np.zeros_like(y[rep+'_'+n+'_molf'])\n",
    "    for i,rep in enumerate(reps):\n",
    "        y[n+'_b6'] += np.array(y[rep+'_'+n+'_b6'])*1e6/N[i]     # get summed RPMs\n",
    "        y[n+'_molf'] += np.array(y[rep+'_'+n+'_molf'])*1e6/N[i] # get summed RPMs\n",
    "        del y[rep+'_'+n+'_b6'], y[rep+'_'+n+'_molf']\n",
    "\n",
    "    y[n] = np.log2( 1+y[n+'_b6']+y[n+'_molf'] )      # normalize summed counts to RPM\n",
    "    del y[n+'_b6'], y[n+'_molf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate X\n",
    "from Bio import SeqIO\n",
    "from bisect import bisect\n",
    "\n",
    "gen_b6 = SeqIO.index(basedir+'f1_hybrid/gen/b6.fa', 'fasta')\n",
    "\n",
    "x = dict()\n",
    "for n,chrom in zip(name,chroms):\n",
    "    x[n] = []\n",
    "    if n!='test' and not summits_neg.empty:\n",
    "        x[n+'_unegs'] = []\n",
    "\n",
    "    for c in tqdm(chrom):        # for each chromosome in train/val/test set\n",
    "        seq_b6 = ''.join(gen_b6.get_raw(str(c)).decode().split('\\n')[1:])\n",
    "\n",
    "        chromsummits = summits.iloc[np.where(summits[0]==c)[0],1]     # slice out the relevant chromosome summits\n",
    "\n",
    "        # get relevant b6 genomic seqs\n",
    "        x[n] += [seq_b6[i-seqlen//2:i+seqlen//2] for i in chromsummits]\n",
    "\n",
    "        if n!='test' and not summits_neg.empty:\n",
    "            chromsummits = summits_neg.loc[np.where(summits_neg[0]==c)[0],1]     # slice out the relevant chromosome summits\n",
    "            x[n+'_unegs'] += [seq_b6[i-seqlen//2:i+seqlen//2] for i in chromsummits]\n",
    "\n",
    "    x[n] = one_hot(x[n])             # convert string of nucleotides to one-hot representation\n",
    "    if n!='test' and not summits_neg.empty:\n",
    "        x[n+'_unegs'] = one_hot(x[n+'_unegs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new code for actual background unegs - can be merged into x code later - SKIP till SKIP END if dont need to compute\n",
    "# written very stupidly, basically just overwrite the old unegs\n",
    "# this is because I didn't want to write logic to do one or the other depending on which method of unegs you want\n",
    "# from utils import get_neg_summits\n",
    "\n",
    "# bamfile = aligndir+b6_strain+'/'+reps[0]+'.combined.rmDup.Aligned.sortedByCoord.out.bam'\n",
    "# bamf = pysam.AlignmentFile(bamfile, \"rb\")\n",
    "\n",
    "# for n,chrom in zip(name,chroms[:2]):\n",
    "#     x[n+'_unegs'] = []\n",
    "    \n",
    "#     for c in tqdm(chrom):        # for each chromosome in train/val/test set\n",
    "#         seq_b6 = ''.join(gen_b6.get_raw(str(c)).decode().split('\\n')[1:])\n",
    "        \n",
    "#         chromsummits = summits.iloc[np.where(summits[0]==c)[0],1]     # slice out the relevant chromosome summits\n",
    "#         neg_summits = np.empty(0, dtype=np.int64)\n",
    "#         while len(neg_summits)<len(chromsummits):     # get neg summits and only keep low coverage ones (<5)\n",
    "#             temp = get_neg_summits(chromsummits, len(chromsummits)-len(neg_summits), len(seq_b6))\n",
    "#             idx = np.where(np.array([bamf.count(str(c),i-seqlen//2,i+seqlen//2) for i in temp]) < 5)[0]\n",
    "#             temp = temp[idx]\n",
    "#             neg_summits = np.concatenate((neg_summits, temp))\n",
    "#         neg_summits = np.sort(neg_summits)\n",
    "        \n",
    "#         # get relevant b6 genomic seqs\n",
    "#         x[n+'_unegs'] += [seq_b6[i-seqlen//2:i+seqlen//2] for i in neg_summits]\n",
    "        \n",
    "#     x[n+'_unegs'] = one_hot(x[n+'_b6_unegs'])\n",
    "# # SKIP END - comment upto here if you want to skip the background sampling and go with flank regions instead\n",
    "\n",
    "gen_b6.close()\n",
    "gen_cast.close()\n",
    "\n",
    "# set uneg log acc to low constant value (at tail end of post-GC residual accessibility distribution)\n",
    "for n,chrom in zip(name[:2],chroms[:2]):\n",
    "    y[n+'_unegs'] = -5.0*np.ones(len(x[n+'_unegs']))\n",
    "\n",
    "# Finally, save everything...\n",
    "if save:\n",
    "    with h5py.File(datadir+'data_noGC'+ident+'_ref.h5','w') as f:\n",
    "        for key in x.keys():\n",
    "            f.create_dataset('x_'+key, data=x[key])\n",
    "        for key in y.keys():\n",
    "            f.create_dataset('y_'+key, data=y[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform GC regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dict()\n",
    "y = dict()\n",
    "with h5py.File(datadir+'data_noGC'+ident+'_ref.h5','r') as f:\n",
    "    for n in name:\n",
    "        x[n] = f['x_'+n][()]\n",
    "        y[n] = f['y_'+n][()]\n",
    "    x['train_unegs'] = f['x_train_unegs'][()]\n",
    "    x['val_unegs'] = f['x_val_unegs'][()]\n",
    "    y['train_unegs'] = f['y_train_unegs'][()]\n",
    "    y['val_unegs'] = f['y_val_unegs'][()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the GC-count relationship\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "temp_x = np.vstack([x[k] for k in x.keys() if 'unegs' not in k])\n",
    "temp_y = np.concatenate([y[k] for k in y.keys() if 'unegs' not in k])\n",
    "GC = np.sum(temp_x[:,(seqlen//2-seqlen1//2):(seqlen//2+seqlen1//2),:], axis=1)\n",
    "GC = GC[:,1]+GC[:,2]\n",
    "ax.scatter(GC/seqlen1*100, temp_y, s=4)\n",
    "coef = GCregress(temp_x[:,(seqlen//2-seqlen1//2):(seqlen//2+seqlen1//2),:], temp_y)    # limit GC counting to only the window where reads are counted\n",
    "temp_y -= coef*GC\n",
    "ax.scatter(GC/seqlen1*100, temp_y, s=4)\n",
    "ax.set_xlabel('GC content (%)')\n",
    "ax.set_ylabel('log accessibility')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regress out GC content\n",
    "temp_x = np.vstack([x[k] for k in x.keys() if 'unegs' not in k])\n",
    "temp_y = np.concatenate([y[k] for k in y.keys() if 'unegs' not in k])\n",
    "coef = GCregress(temp_x[:,(seqlen//2-seqlen1//2):(seqlen//2+seqlen1//2),:], temp_y)    # limit GC counting to only the window where reads are counted\n",
    "\n",
    "\n",
    "cnt=0\n",
    "del x['test'], y['test']   # don't need these since test set comes from 'both' anyways\n",
    "for n in ['train','val']:\n",
    "    GC = np.sum(x[n][:,(seqlen//2-seqlen1//2):(seqlen//2+seqlen1//2),:], axis=1)\n",
    "    GC = GC[:,1]+GC[:,2]\n",
    "    y[n] -= coef*GC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do uneg count adjustment based on range of GC corrected values\n",
    "# print(np.min(y['train']))\n",
    "# print(np.unique(y['train_unegs']))\n",
    "# y['train_unegs'] -= 5\n",
    "# y['val_unegs'] -= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save:\n",
    "    with h5py.File(datadir+'data'+ident+'_ref.h5','w') as f:\n",
    "        for key in x.keys():\n",
    "            f.create_dataset('x_'+key, data=x[key])\n",
    "            f.create_dataset('y_'+key, data=y[key])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f07b599dba35ea3e3431fe483080f9a2b540e1c33071a57a1e300447d2107f4"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
