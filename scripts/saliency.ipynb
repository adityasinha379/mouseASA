{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saliency analysis using Captum\n",
    "https://captum.ai/\n",
    "\n",
    "Deeplift used for plotting only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:24:37.378670Z",
     "start_time": "2022-03-07T18:24:37.371071Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch import autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import gaussian_kde\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "from captum.attr import IntegratedGradients\n",
    "import sys\n",
    "sys.path.append('/data/leslie/gaov/programs/deeplift')\n",
    "import deeplift\n",
    "from deeplift.visualization import viz_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:23:21.944137Z",
     "start_time": "2022-03-07T18:23:21.936249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Path to the trained model\n",
    "SAVEPATH = '/data/leslie/gaov/project/MouseASA/ckpt_models/model_3.0_transformer/cd8/cd8_Model3.2_fourier_both_batchsize128.hdf5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:23:21.970182Z",
     "start_time": "2022-03-07T18:23:21.948480Z"
    },
    "code_folding": [
     0,
     8,
     21,
     35
    ]
   },
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "    \n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "    \n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:23:21.994078Z",
     "start_time": "2022-03-07T18:23:21.973850Z"
    },
    "code_folding": [
     0,
     18
    ]
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:23:22.012176Z",
     "start_time": "2022-03-07T18:23:21.997188Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:23:22.029129Z",
     "start_time": "2022-03-07T18:23:22.015219Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class resblock(nn.Module):\n",
    "    def __init__(self,ni):\n",
    "        super(resblock, self).__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Conv1d(ni, ni, 3, 1, 1),\n",
    "            nn.BatchNorm1d(ni),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(ni, ni, 1, 1, 0),\n",
    "            nn.BatchNorm1d(ni),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        residual = x\n",
    "        out = self.blocks(x)        \n",
    "        out += residual\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:23:51.624347Z",
     "start_time": "2022-03-07T18:23:51.589066Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "class allele_scan(nn.Module):\n",
    "\n",
    "    def __init__(self, poolsize, dropout):\n",
    "        super(allele_scan, self).__init__()\n",
    "        self.poolsize = poolsize\n",
    "        self.dropout = dropout\n",
    "        n = 32\n",
    "        n2 = 16\n",
    "        hidden=n\n",
    "        n_layers=4\n",
    "        attn_heads=4\n",
    "        \n",
    "        self.seq_extractor = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 4, out_channels = n2, kernel_size = 15, stride = 1, dilation = 1, padding = 7),\n",
    "            nn.BatchNorm1d(n2,eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv1d(in_channels = n2, out_channels = n, kernel_size = 11, stride = 1, dilation = 1, padding = 5),\n",
    "            nn.BatchNorm1d(n,eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "        self.seq_extractor_2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = n, out_channels = n2, kernel_size = 5, stride = 1, dilation = 1, padding = 2),\n",
    "            nn.BatchNorm1d(n2,eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = self.poolsize),  \n",
    "\n",
    "            nn.Conv1d(in_channels = n2, out_channels = n2, kernel_size = 5, stride = 1, dilation = 1, padding = 2),\n",
    "            nn.BatchNorm1d(n2,eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = self.poolsize),  \n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(in_features=(1200), out_features=300),\n",
    "            nn.BatchNorm1d(300,eps=1e-3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=(300), out_features=1),\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.seq_extractor(x)\n",
    "#         print('1_{}'.format(x.shape))\n",
    "        x = x.permute(0,2,1)\n",
    "        \n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, None)\n",
    "#         print('2_{}'.format(x.shape))\n",
    "        x = x.permute(0,2,1)\n",
    "#         print('3_{}'.format(x.shape))\n",
    "        x = self.seq_extractor_2(x)\n",
    "#         print('3_{}'.format(x.shape))\n",
    "\n",
    "        x = torch.flatten(x,1)\n",
    "#         print('flatten_{}'.format(x.shape))\n",
    "        \n",
    "        x = self.dense(x)\n",
    "#         print('dense_{}'.format(x.shape))\n",
    "\n",
    "        x = torch.flatten(x)\n",
    "#         print('pred_{}'.format(x.shape))\n",
    "        return x\n",
    "    \n",
    "    def fourier_att_prior_loss(\n",
    "        self, status, input_grads, freq_limit, limit_softness,\n",
    "        att_prior_grad_smooth_sigma\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Computes an attribution prior loss for some given training examples,\n",
    "        using a Fourier transform form.\n",
    "        Arguments:\n",
    "            `status`: a B-tensor, where B is the batch size; each entry is 1 if\n",
    "                that example is to be treated as a positive example, and 0\n",
    "                otherwise\n",
    "            `input_grads`: a B x L x 4 tensor, where B is the batch size, L is\n",
    "                the length of the input; this needs to be the gradients of the\n",
    "                input with respect to the output; this should be\n",
    "                *gradient times input*\n",
    "            `freq_limit`: the maximum integer frequency index, k, to consider for\n",
    "                the loss; this corresponds to a frequency cut-off of pi * k / L;\n",
    "                k should be less than L / 2\n",
    "            `limit_softness`: amount to soften the limit by, using a hill\n",
    "                function; None means no softness\n",
    "            `att_prior_grad_smooth_sigma`: amount to smooth the gradient before\n",
    "                computing the loss\n",
    "        Returns a single scalar Tensor consisting of the attribution loss for\n",
    "        the batch.\n",
    "        \"\"\"\n",
    "        abs_grads = torch.sum(torch.abs(input_grads), dim=2)\n",
    "\n",
    "        # Smooth the gradients\n",
    "        grads_smooth = smooth_tensor_1d(\n",
    "            abs_grads, att_prior_grad_smooth_sigma\n",
    "        )\n",
    "\n",
    "        # Only do the positives\n",
    "        pos_grads = grads_smooth[status == 1]\n",
    "\n",
    "        # Loss for positives\n",
    "        if pos_grads.nelement():\n",
    "            pos_fft = torch.rfft(pos_grads, 1)\n",
    "            pos_mags = torch.norm(pos_fft, dim=2)\n",
    "            pos_mag_sum = torch.sum(pos_mags, dim=1, keepdim=True)\n",
    "            pos_mag_sum[pos_mag_sum == 0] = 1  # Keep 0s when the sum is 0\n",
    "            pos_mags = pos_mags / pos_mag_sum\n",
    "\n",
    "            # Cut off DC\n",
    "            pos_mags = pos_mags[:, 1:]\n",
    "\n",
    "            # Construct weight vector\n",
    "            weights = place_tensor(torch.ones_like(pos_mags))\n",
    "            if limit_softness is None:\n",
    "                weights[:, freq_limit:] = 0\n",
    "            else:\n",
    "                x = place_tensor(\n",
    "                    torch.arange(1, pos_mags.size(1) - freq_limit + 1)\n",
    "                ).float()\n",
    "                weights[:, freq_limit:] = 1 / (1 + torch.pow(x, limit_softness))\n",
    "\n",
    "            # Multiply frequency magnitudes by weights\n",
    "            pos_weighted_mags = pos_mags * weights\n",
    "\n",
    "            # Add up along frequency axis to get score\n",
    "            pos_score = torch.sum(pos_weighted_mags, dim=1)\n",
    "            pos_loss = 1 - pos_score\n",
    "            return torch.mean(pos_loss)\n",
    "        else:\n",
    "            return place_tensor(torch.zeros(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:23:56.840868Z",
     "start_time": "2022-03-07T18:23:53.217518Z"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "initial_rate = 1e-3\n",
    "wd = 1e-3\n",
    "poolsize = 2\n",
    "dropout = 0.2\n",
    "N_EPOCHS = 100\n",
    "RANDOM_SEED = 0\n",
    "use_prior = True\n",
    "dataset = 'both'\n",
    "#fourier param\n",
    "freq_limit = 150\n",
    "limit_softness = 0.2\n",
    "att_prior_grad_smooth_sigma = 3\n",
    "\n",
    "model = allele_scan(poolsize, dropout)\n",
    "model.to(DEVICE)\n",
    "model.load_state_dict(torch.load(SAVEPATH))\n",
    "model.eval()\n",
    "loss_fcn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:26:16.488466Z",
     "start_time": "2022-03-07T18:26:14.660747Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data_noclip(dataset):\n",
    "    xTr = np.load('/data/leslie/shared/forAditya/MouseASA/old_data/cd8/Xtr_{}_set2.npz'.format(dataset))\n",
    "    yTr = np.load('/data/leslie/shared/forAditya/MouseASA/old_data/cd8/Ytr_{}_set2.npz'.format(dataset))\n",
    "    xTe = np.load('/data/leslie/shared/forAditya/MouseASA/old_data/cd8/Xte_{}_set2.npz'.format(dataset))\n",
    "    yTe = np.load('/data/leslie/shared/forAditya/MouseASA/old_data/cd8/Yte_{}_set2.npz'.format(dataset))\n",
    "    xVa = np.load('/data/leslie/shared/forAditya/MouseASA/old_data/cd8/Xva_{}_set2.npz'.format(dataset))\n",
    "    yVa = np.load('/data/leslie/shared/forAditya/MouseASA/old_data/cd8/Yva_{}_set2.npz'.format(dataset))\n",
    "    return xTr['arr_0'], xTe['arr_0'], xVa['arr_0'], yTr['arr_0'], yTe['arr_0'], yVa['arr_0']\n",
    "\n",
    "x_train, x_test, x_valid, y_train, y_test, y_valid = load_data_noclip(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integrated gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:26:17.910891Z",
     "start_time": "2022-03-07T18:26:17.907776Z"
    }
   },
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:32:55.889806Z",
     "start_time": "2022-03-07T18:32:55.066951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run deeplift on all test samples\n",
    "# x_test: one-hot encoded 300bp sequence inputs with shape (n, 300, 4)\n",
    "attr_list = []\n",
    "x_test_b6 = x_test[0:3637]\n",
    "x_test_ca = x_test[3637:]\n",
    "deeplift_samples = np.concatenate([x_test_b6, x_test_ca], axis = 0)\n",
    "\n",
    "for s in tqdm(deeplift_samples): # only look at the first 10 peaks for both b6 and cast\n",
    "    test_input_tensor = torch.from_numpy(s).unsqueeze(0).type(torch.FloatTensor).transpose(1,2).to(DEVICE)\n",
    "    test_input_tensor.requires_grad_()\n",
    "    attr, delta = ig.attribute(test_input_tensor,return_convergence_delta=True)\n",
    "    attr = attr.detach().cpu().numpy()\n",
    "    attr_list.append(attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:32:55.911354Z",
     "start_time": "2022-03-07T18:32:55.895447Z"
    },
    "code_folding": [
     7
    ]
   },
   "outputs": [],
   "source": [
    "def paired(s1, s2):\n",
    "    '''\n",
    "    Function to find differences in a pair of sequences s1 and s2\n",
    "    '''\n",
    "    sm, index_ch = 0, []\n",
    "    for ind, (c1, c2) in enumerate(zip(s1, s2)):\n",
    "        if c1 != c2:\n",
    "            sm += 1\n",
    "            index_ch.append((ind, c1))\n",
    "    return index_ch, sm\n",
    "def hot1_dna(seqs_1hot):\n",
    "  \"\"\" Convert 1-hot coded sequences to ACGTN. \"\"\"\n",
    "\n",
    "  singleton = False\n",
    "  if seqs_1hot.ndim == 2:\n",
    "    singleton = True\n",
    "    seqs_1hot = np.expand_dims(seqs_1hot, 0)\n",
    "\n",
    "  seqs = []\n",
    "  for si in range(seqs_1hot.shape[0]):\n",
    "    seq_list = ['A'] * seqs_1hot.shape[1]\n",
    "    for li in range(seqs_1hot.shape[1]):\n",
    "      if seqs_1hot[si, li, 0] == 1:\n",
    "        seq_list[li] = 'A'\n",
    "      elif seqs_1hot[si, li, 1] == 1:\n",
    "        seq_list[li] = 'C'\n",
    "      elif seqs_1hot[si, li, 2] == 1:\n",
    "        seq_list[li] = 'G'\n",
    "      elif seqs_1hot[si, li, 3] == 1:\n",
    "        seq_list[li] = 'T'\n",
    "      else:\n",
    "        seq_list[li] = 'N'\n",
    "\n",
    "    seqs.append(''.join(seq_list))\n",
    "\n",
    "  if singleton:\n",
    "    seqs = seqs[0]\n",
    "\n",
    "  return seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-07T18:36:37.044704Z",
     "start_time": "2022-03-07T18:33:43.979956Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot deeplift results\n",
    "\n",
    "method_name = 'Integrated Gradient'\n",
    "n_to_plot = 5\n",
    "# I'm only plotting the first 10 pairs of examples here. You can filter for the ones with significant changes in accessibility.\n",
    "for idx in range(n_to_plot):\n",
    "    seq_b6 = hot1_dna(deeplift_samples[idx])\n",
    "    seq_ca = hot1_dna(deeplift_samples[idx+3637])\n",
    "    index_char, sm = paired(seq_b6, seq_ca)\n",
    "    if sm<20:\n",
    "        mut_loc_b6 = [x[0] for x in index_char]\n",
    "        mut_loc_cast = mut_loc_b6\n",
    "    else:\n",
    "        mut_loc_b6 = ['']\n",
    "        mut_loc_cast = ['']\n",
    "\n",
    "    print('SNP/Indel location:',index_char)\n",
    "\n",
    "    scores_for_idx = attr_list[idx][0]\n",
    "    original_onehot = deeplift_samples[idx]\n",
    "    scores_for_idx = original_onehot*scores_for_idx.T#[:,None]\n",
    "    scores_for_idx_ca = attr_list[idx+3637][0]\n",
    "    original_onehot_ca = deeplift_samples[idx+3637]\n",
    "    scores_for_idx_ca = original_onehot_ca*scores_for_idx_ca.T#[:,None]\n",
    "    print('b6     ', method_name)\n",
    "    viz_sequence.plot_weights(scores_for_idx, subticks_frequency=10, \n",
    "                              figsize = (30,4))\n",
    "    print('ca     ', method_name)\n",
    "    viz_sequence.plot_weights(scores_for_idx_ca, subticks_frequency=10, \n",
    "                              figsize = (30,4) )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
